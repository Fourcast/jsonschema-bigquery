#!/usr/bin/env node

const jsbq = module.exports = {}

const gbq = require('../src/gbq')
const util = require('../src/util')
const converter = require('../src/converter')
const logger = require('log-driver').logger
const fs = require('fs')
const { promisify } = require('util')

jsbq.process = async (project, datasetName, jsonSchema) => {
  logger.info('Processing json schema...')
  const tableOptions = converter.run(jsonSchema)
  logger.info(JSON.stringify(tableOptions))

  if (!project && !datasetName) {
    return logger.info('Skipping table operations')
  }

  logger.info('Extracting table name from schema...')
  const tableName = util.get_table_id(jsonSchema.id)
  logger.info('Table name is ' + tableName)

  logger.info('Setting table options...')
  tableOptions.friendly_name = jsonSchema.title
  tableOptions.description = jsonSchema.description || jsonSchema.title
  tableOptions.timePartitioning = {
    'type': 'DAY'
  }

  logger.info('Checking if table exists...')
  const tableExists = await gbq.tableExists(project, datasetName, tableName)
  if (tableExists) {
    logger.info('Patching table ' + tableName)
    await gbq.patchTable(project, datasetName, tableName, tableOptions)
  } else {
    logger.info('Creating table ' + tableName)
    await gbq.createTable(project, datasetName, tableName, tableOptions)
  }
  logger.info('Finished')
}

jsbq.run = async () => {
  const readFile = promisify(fs.readFile)

  const argv = require('yargs')
  .usage('Usage: $0 -p [project] -d [dataset] -j <json schema file>')
  .options({
    j: {
      describe: 'JSON schema file',
      demandOption: true
    },
    p: {
      describe: 'GCP project name'
    },
    d: {
      describe: 'BigQuery dataset name'
    }
  })
  .argv

  const schemaData = await readFile(argv.j)
  const jsonSchema = JSON.parse(schemaData)
  logger.debug('Schema is: ' + JSON.stringify(jsonSchema))
  await jsbq.process(argv.p, argv.d, jsonSchema)
}

jsbq.run()
